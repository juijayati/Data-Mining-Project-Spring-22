{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import read_genes_from_file\n",
    "\n",
    "all_genes = read_genes_from_file(\"./GeneLists/all_genes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3749\n",
      "['VEGFA', 'TGFB1', 'TNF', 'AKT1', 'EGF', 'FGF2', 'FN1', 'MMP9', 'MMP2', 'IL6']\n"
     ]
    }
   ],
   "source": [
    "# find how many genes can be mapped to symbol to create the PPI dataset using R\n",
    "mapping_df = pd.read_excel(\"./GeneLists/mapping.xlsx\", engine=\"openpyxl\")\n",
    "eids = mapping_df.Entrenz.to_numpy()\n",
    "syms = []\n",
    "for gene in all_genes:\n",
    "    inds = np.where(eids == gene)[0]\n",
    "    if(len(inds) > 0):\n",
    "        syms.append(mapping_df.iloc[inds[0],0])\n",
    "print(len(syms))\n",
    "print(syms[0:10])\n",
    "\n",
    "with open(\"mapped_symbols.txt\",\"w\") as f:\n",
    "    for sym in syms:\n",
    "        f.write(sym + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      from       to  combined_score  label\n",
      "0     CFTR     DVL2             951   True\n",
      "1    XYLT2  B4GALT7             977   True\n",
      "2    XYLT2      DCN             924   True\n",
      "3  B4GALT7      DCN             940   True\n",
      "4     CFTR    VAMP3             911   True\n",
      "    Symbol      Entrenz\n",
      "0     A1BG       Gene_1\n",
      "1     A1CF   Gene_29974\n",
      "2      A2M       Gene_2\n",
      "3    A2ML1  Gene_144568\n",
      "4  A3GALT2  Gene_127550\n",
      "         from          to  combined_score  label\n",
      "0   Gene_1080   Gene_1856             951   True\n",
      "1  Gene_64132  Gene_11285             977   True\n",
      "2  Gene_64132   Gene_1634             924   True\n",
      "3  Gene_11285   Gene_1634             940   True\n",
      "4   Gene_1080   Gene_9341             911   True\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Datasets/PPI.tsv\", sep = \"\\t\")\n",
    "print(data.head())\n",
    "mapping_df = pd.read_excel(\"./GeneLists/mapping.xlsx\", engine=\"openpyxl\")\n",
    "print(mapping_df.head())\n",
    "syms = mapping_df.Symbol.to_numpy()\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    gene1 = row['from']\n",
    "    gene2 = row['to']\n",
    "    data.iloc[index,0] = mapping_df.iloc[np.where(syms == gene1)[0][0],1]\n",
    "    data.iloc[index,1] = mapping_df.iloc[np.where(syms == gene2)[0][0],1]\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"PPI.tsv\", index = False, sep =\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding loaded from ./WordVectors/Computed/word2vec_cbow.bin\n",
      "Time to load cbow embeddings in mins:  0.0036\n",
      "embedding loaded from ./WordVectors/Computed/word2vec_skipgram.bin\n",
      "Time to load skipgram embeddings in mins:  0.0032\n",
      "(57126, 4)\n",
      "(55200, 4)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models  import KeyedVectors, Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import load_embedding\n",
    "from time import time\n",
    "\n",
    "t = time()\n",
    "w2v_cbow = load_embedding(\"./WordVectors/Computed/word2vec_cbow.bin\", binary=True)\n",
    "print(\"Time to load cbow embeddings in mins: \", round(((time() - t)/60.0),4))\n",
    "\n",
    "t = time()\n",
    "w2v_sg = load_embedding(\"./WordVectors/Computed/word2vec_skipgram.bin\", binary=True)\n",
    "print(\"Time to load skipgram embeddings in mins: \", round(((time() - t)/60.0),4))\n",
    "\n",
    "data = pd.read_csv(\"./Datasets/PPI.tsv\", sep = \"\\t\")\n",
    "X = data.iloc[:,0:2]\n",
    "y = data.label.to_numpy().tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1234, test_size=0.2)\n",
    "\n",
    "print(data[data.label == True].shape)\n",
    "print(data[data.label == False].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7433494848738598, 0.6750857096005907, 0.6470819960558369, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.61      0.94      0.74     11027\n",
      "       False       0.88      0.41      0.56     11439\n",
      "\n",
      "    accuracy                           0.67     22466\n",
      "   macro avg       0.74      0.68      0.65     22466\n",
      "weighted avg       0.75      0.67      0.65     22466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from utils import cosine\n",
    "\n",
    "model = w2v_cbow\n",
    "\n",
    "thr = 0.5\n",
    "\n",
    "predictions = []\n",
    "for index, row in X_test.iterrows():\n",
    "    sim = cosine(model[row['from']], model[row['to']])\n",
    "    if sim > thr:\n",
    "        predictions.append(True)\n",
    "    else:\n",
    "        predictions.append(False)\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, predictions, average='macro'))\n",
    "target_names = [\"True\", \"False\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (89860, 200)\n",
      "Test shape:  (22466, 200)\n"
     ]
    }
   ],
   "source": [
    "X_train_vecs = []\n",
    "for index, row in X_train.iterrows():\n",
    "    X_train_vecs.append(np.hstack([model[row['from']], model[row['to']]]))\n",
    "X_train_vecs = np.vstack(X_train_vecs)\n",
    "\n",
    "X_test_vecs = []\n",
    "for index, row in X_test.iterrows():\n",
    "    X_test_vecs.append(np.hstack([model[row['from']], model[row['to']]]))\n",
    "X_test_vecs = np.vstack(X_test_vecs)\n",
    "\n",
    "print(\"Train shape: \", X_train_vecs.shape)\n",
    "print(\"Test shape: \", X_test_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.695112362059394, 0.69504430204627, 0.6947340325483564, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.68      0.71      0.70     11027\n",
      "       False       0.71      0.68      0.69     11439\n",
      "\n",
      "    accuracy                           0.69     22466\n",
      "   macro avg       0.70      0.70      0.69     22466\n",
      "weighted avg       0.70      0.69      0.69     22466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000, solver = \"saga\", penalty=\"l1\", C = 5).fit(X_train_vecs, y_train)\n",
    "predictions = clf.predict(X_test_vecs)\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, predictions, average='macro'))\n",
    "target_names = [\"True\", \"False\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6423230413410772, 0.6385176145339972, 0.6350128003189133, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.61      0.72      0.66     11027\n",
      "       False       0.67      0.55      0.61     11439\n",
      "\n",
      "    accuracy                           0.64     22466\n",
      "   macro avg       0.64      0.64      0.64     22466\n",
      "weighted avg       0.64      0.64      0.63     22466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(X_train_vecs, y_train)\n",
    "predictions = gnb.predict(X_test_vecs)\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, predictions, average='macro'))\n",
    "target_names = [\"True\", \"False\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8979244712599455, 0.8973162837962685, 0.8974815777312427, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.91      0.88      0.89     11027\n",
      "       False       0.89      0.91      0.90     11439\n",
      "\n",
      "    accuracy                           0.90     22466\n",
      "   macro avg       0.90      0.90      0.90     22466\n",
      "weighted avg       0.90      0.90      0.90     22466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C = 5, kernel='rbf').fit(X_train_vecs, y_train)\n",
    "predictions = clf.predict(X_test_vecs)\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, predictions, average='macro'))\n",
    "target_names = [\"True\", \"False\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:49:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "(0.8958392282622161, 0.8957667370476015, 0.8957985403402515, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.90      0.89      0.89     11027\n",
      "       False       0.90      0.90      0.90     11439\n",
      "\n",
      "    accuracy                           0.90     22466\n",
      "   macro avg       0.90      0.90      0.90     22466\n",
      "weighted avg       0.90      0.90      0.90     22466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
    "       min_child_weight=1, n_estimators=500, nthread=-1,\n",
    "       objective='binary:logistic', seed=0, silent=True, subsample=1)\n",
    "\n",
    "xgb.fit(X_train_vecs, y_train)\n",
    "predictions = xgb.predict(X_test_vecs)\n",
    "predictions = [True if value > 0.5 else False for value in predictions]\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, predictions, average='macro'))\n",
    "target_names = [\"True\", \"False\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7454506530087759, 0.6812419187125375, 0.6555267053725609, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.61      0.94      0.74     11027\n",
      "       False       0.88      0.42      0.57     11439\n",
      "\n",
      "    accuracy                           0.68     22466\n",
      "   macro avg       0.75      0.68      0.66     22466\n",
      "weighted avg       0.75      0.68      0.65     22466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = w2v_sg\n",
    "\n",
    "thr = 0.5\n",
    "\n",
    "predictions = []\n",
    "for index, row in X_test.iterrows():\n",
    "    sim = cosine(model[row['from']], model[row['to']])\n",
    "    if sim > thr:\n",
    "        predictions.append(True)\n",
    "    else:\n",
    "        predictions.append(False)\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, predictions, average='macro'))\n",
    "target_names = [\"True\", \"False\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (89860, 200)\n",
      "Test shape:  (22466, 200)\n"
     ]
    }
   ],
   "source": [
    "X_train_vecs = []\n",
    "for index, row in X_train.iterrows():\n",
    "    X_train_vecs.append(np.hstack([model[row['from']], model[row['to']]]))\n",
    "X_train_vecs = np.vstack(X_train_vecs)\n",
    "\n",
    "X_test_vecs = []\n",
    "for index, row in X_test.iterrows():\n",
    "    X_test_vecs.append(np.hstack([model[row['from']], model[row['to']]]))\n",
    "X_test_vecs = np.vstack(X_test_vecs)\n",
    "\n",
    "print(\"Train shape: \", X_train_vecs.shape)\n",
    "print(\"Test shape: \", X_test_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6975858608201462, 0.6975504767787668, 0.6972744434670354, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.68      0.71      0.70     11027\n",
      "       False       0.71      0.68      0.70     11439\n",
      "\n",
      "    accuracy                           0.70     22466\n",
      "   macro avg       0.70      0.70      0.70     22466\n",
      "weighted avg       0.70      0.70      0.70     22466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000, solver = \"saga\", penalty=\"l1\", C = 5).fit(X_train_vecs, y_train)\n",
    "predictions = clf.predict(X_test_vecs)\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, predictions, average='macro'))\n",
    "target_names = [\"True\", \"False\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6453693901691318, 0.6422387378037899, 0.6393221964982218, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.61      0.72      0.66     11027\n",
      "       False       0.68      0.57      0.62     11439\n",
      "\n",
      "    accuracy                           0.64     22466\n",
      "   macro avg       0.65      0.64      0.64     22466\n",
      "weighted avg       0.65      0.64      0.64     22466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(X_train_vecs, y_train)\n",
    "predictions = gnb.predict(X_test_vecs)\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, predictions, average='macro'))\n",
    "target_names = [\"True\", \"False\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "(0.8975643408611877, 0.8974293545332502, 0.8974836781799639, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.90      0.89      0.90     11027\n",
      "       False       0.90      0.90      0.90     11439\n",
      "\n",
      "    accuracy                           0.90     22466\n",
      "   macro avg       0.90      0.90      0.90     22466\n",
      "weighted avg       0.90      0.90      0.90     22466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
    "       min_child_weight=1, n_estimators=500, nthread=-1,\n",
    "       objective='binary:logistic', seed=0, silent=True, subsample=1)\n",
    "\n",
    "xgb.fit(X_train_vecs, y_train)\n",
    "predictions = xgb.predict(X_test_vecs)\n",
    "predictions = [True if value > 0.5 else False for value in predictions]\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, predictions, average='macro'))\n",
    "target_names = [\"True\", \"False\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8991822038683968, 0.8985597646092802, 0.8987279722405602, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.91      0.88      0.90     11027\n",
      "       False       0.89      0.91      0.90     11439\n",
      "\n",
      "    accuracy                           0.90     22466\n",
      "   macro avg       0.90      0.90      0.90     22466\n",
      "weighted avg       0.90      0.90      0.90     22466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C = 5, kernel='rbf').fit(X_train_vecs, y_train)\n",
    "predictions = clf.predict(X_test_vecs)\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, predictions, average='macro'))\n",
    "target_names = [\"True\", \"False\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c94c8a2b4a38d84c9d502ef75fc1564633a6cf3f568e3899ec7413ad0d1be19"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
